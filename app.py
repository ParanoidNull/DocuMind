import streamlit as st
from pypdf import PdfReader
import os
from dotenv import load_dotenv

# --- Modern LangChain ImportlarÄ± (DÃ¼zeltilmiÅŸ) ---
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_openai import OpenAIEmbeddings, ChatOpenAI
    from langchain_community.vectorstores import FAISS
    
except ImportError as e:
    st.error(f"Kritik KÃ¼tÃ¼phane HatasÄ±: {e}")
    st.error("LÃ¼tfen ÅŸu komutu Ã§alÄ±ÅŸtÄ±rÄ±n: python -m pip install langchain langchain-community langchain-openai pypdf python-dotenv streamlit faiss-cpu")
    st.stop()

# --- Configuration & Setup ---
load_dotenv()

# Streamlit Page Config
st.set_page_config(
    page_title="DocuMind | AI Research Assistant",
    page_icon="ğŸ§ ",
    layout="wide"
)

# --- Core Functions ---

def extract_text_from_pdf(pdf_docs):
    """
    PDF dosyalarÄ±ndan ham metni Ã§Ä±karÄ±r.
    """
    text = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text
    return text

def get_text_chunks(text):
    """
    Metni iÅŸlenebilir parÃ§alara bÃ¶ler.
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    return chunks

def create_vector_store(text_chunks, api_key):
    """
    Embedding oluÅŸturur ve FAISS veritabanÄ±na kaydeder.
    """
    embeddings = OpenAIEmbeddings(openai_api_key=api_key)
    vector_store = FAISS.from_texts(texts=text_chunks, embedding=embeddings)
    vector_store.save_local("faiss_index")
    return vector_store

def process_user_input(user_question, api_key):
    """
    KullanÄ±cÄ± sorusunu iÅŸler ve cevap dÃ¶ndÃ¼rÃ¼r.
    Modern ve basit yaklaÅŸÄ±m - load_qa_chain kullanmÄ±yor.
    """
    try:
        embeddings = OpenAIEmbeddings(openai_api_key=api_key)
        
        # FAISS veritabanÄ±nÄ± yÃ¼kle
        new_db = FAISS.load_local(
            "faiss_index", 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        
        # En alakalÄ± 3 dokÃ¼manÄ± bul
        docs = new_db.similarity_search(user_question, k=3)
        
        # LLM'i baÅŸlat
        llm = ChatOpenAI(
            model="gpt-3.5-turbo", 
            temperature=0.3, 
            openai_api_key=api_key
        )
        
        # Context'i birleÅŸtir
        context = "\n\n---\n\n".join([doc.page_content for doc in docs])
        
        # Prompt oluÅŸtur
        prompt = f"""AÅŸaÄŸÄ±daki baÄŸlam bilgisine dayanarak soruyu yanÄ±tla. 
EÄŸer cevabÄ± baÄŸlamda bulamazsan, bilmediÄŸini sÃ¶yle. Uydurma.

BAÄLAM:
{context}

SORU: {user_question}

CEVAP:"""
        
        # CevabÄ± al
        response = llm.invoke(prompt)
        
        return response.content
        
    except Exception as e:
        raise Exception(f"Ä°ÅŸlem hatasÄ±: {str(e)}")

# --- Main UI Logic ---

def main():
    st.header("ğŸ§  DocuMind: Chat with your Documents")
    
    # Sidebar
    with st.sidebar:
        st.title("âš™ï¸ Settings")
        st.markdown("---")
        
        api_key = st.text_input(
            "OpenAI API Key", 
            type="password", 
            help="Enter your OpenAI API key (starts with sk-)"
        )
        
        st.subheader("ğŸ“„ Your Documents")
        pdf_docs = st.file_uploader(
            "Upload PDFs here and click 'Process'", 
            accept_multiple_files=True,
            type=['pdf']
        )
        
        if st.button("ğŸ”„ Process Documents", use_container_width=True):
            if not api_key:
                st.error("âš ï¸ Please enter your OpenAI API Key first.")
            elif not pdf_docs:
                st.warning("âš ï¸ Please upload at least one PDF.")
            else:
                with st.spinner("ğŸ” Analyzing documents... This may take a moment."):
                    try:
                        # 1. Extract
                        raw_text = extract_text_from_pdf(pdf_docs)
                        
                        if not raw_text.strip():
                            st.error("âŒ No text could be extracted from the PDFs.")
                            return
                        
                        # 2. Split
                        text_chunks = get_text_chunks(raw_text)
                        st.info(f"ğŸ“Š Created {len(text_chunks)} text chunks")
                        
                        # 3. Vectorize
                        create_vector_store(text_chunks, api_key)
                        
                        st.success("âœ… Indexing Complete! You can now ask questions.")
                        
                    except Exception as e:
                        st.error(f"âŒ Processing Error: {e}")
        
        # Info box
        with st.expander("â„¹ï¸ How to use"):
            st.markdown("""
            1. Enter your OpenAI API key
            2. Upload one or more PDF files
            3. Click 'Process Documents'
            4. Ask questions about your documents
            """)

    # Main Chat Area
    st.markdown("### ğŸ’¬ Ask Questions")
    user_question = st.text_input(
        "Type your question here:",
        placeholder="e.g., What are the main points discussed in the document?"
    )

    if user_question:
        if not api_key:
            st.error("ğŸ”‘ API Key is missing. Please enter it in the sidebar.")
        elif not os.path.exists("faiss_index"):
            st.warning("ğŸ“ Please upload and process documents first.")
        else:
            with st.spinner("ğŸ¤” Thinking..."):
                try:
                    response = process_user_input(user_question, api_key)
                    
                    st.markdown("### ğŸ“ Answer")
                    st.write(response)
                    
                    with st.expander("ğŸ” View Source Context"):
                        st.info("The answer was derived from the most relevant sections of your uploaded PDF documents.")
                        
                except Exception as e:
                    st.error(f"âŒ An error occurred: {e}")
                    st.info("ğŸ’¡ Tip: Make sure your API key is valid and you have processed documents first.")

    # Footer
    st.markdown("---")
    st.markdown(
        "<div style='text-align: center; color: gray;'>Made with â¤ï¸ using Streamlit & LangChain</div>", 
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()